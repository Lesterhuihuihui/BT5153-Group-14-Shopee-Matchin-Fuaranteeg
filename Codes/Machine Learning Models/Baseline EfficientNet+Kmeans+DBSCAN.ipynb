{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T14:16:04.796952Z","iopub.status.busy":"2022-04-20T14:16:04.796594Z","iopub.status.idle":"2022-04-20T14:16:10.590895Z","shell.execute_reply":"2022-04-20T14:16:10.590077Z","shell.execute_reply.started":"2022-04-20T14:16:04.796873Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T14:16:10.593320Z","iopub.status.busy":"2022-04-20T14:16:10.592941Z","iopub.status.idle":"2022-04-20T14:16:10.792391Z","shell.execute_reply":"2022-04-20T14:16:10.791472Z","shell.execute_reply.started":"2022-04-20T14:16:10.593283Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(34250, 5)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>posting_id</th>\n","      <th>image</th>\n","      <th>image_phash</th>\n","      <th>title</th>\n","      <th>label_group</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_129225211</td>\n","      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n","      <td>94974f937d4c2433</td>\n","      <td>Paper Bag Victoria Secret</td>\n","      <td>249114794</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_3386243561</td>\n","      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n","      <td>af3f9460c2838f0f</td>\n","      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n","      <td>2937985045</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_2288590299</td>\n","      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n","      <td>b94cb00ed3e50f78</td>\n","      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n","      <td>2395904891</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         posting_id                                 image       image_phash  \\\n","0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n","1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n","2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n","\n","                                               title  label_group  \n","0                          Paper Bag Victoria Secret    249114794  \n","1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  \n","2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["train   = pd.read_csv('../input/shopee-product-matching/train.csv')\n","test    = pd.read_csv('../input/shopee-product-matching/test.csv')\n","\n","print(train.shape)\n","train.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Metics(F1 Score)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def f1_score(col):\n","    def score(row):\n","        intersection = len(np.intersect1d(row['target'], row[col]))\n","        return 2 * intersection / (len(row['target']) + len(row[col]))\n","    return score"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T14:16:10.794589Z","iopub.status.busy":"2022-04-20T14:16:10.793919Z","iopub.status.idle":"2022-04-20T14:16:16.321084Z","shell.execute_reply":"2022-04-20T14:16:16.320151Z","shell.execute_reply.started":"2022-04-20T14:16:10.794535Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train F1 Score - method:pHash = 0.5530933399167943\n"]}],"source":["pid_mapping = train.groupby('image_phash')['posting_id'].agg('unique').to_dict()\n","train['matches_phash'] = train['image_phash'].map(pid_mapping)\n","\n","train['f1_phash'] = train.apply(f1_score('matches_phash'), axis=1)\n","print('Train F1 Score - method:pHash =', train['f1_phash'].mean())"]},{"cell_type":"markdown","metadata":{},"source":["## EfficientNet Embedding"]},{"cell_type":"code","execution_count":4,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-20T14:16:16.322692Z","iopub.status.busy":"2022-04-20T14:16:16.322349Z","iopub.status.idle":"2022-04-20T14:16:16.331971Z","shell.execute_reply":"2022-04-20T14:16:16.330979Z","shell.execute_reply.started":"2022-04-20T14:16:16.322656Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import cv2\n","\n","class DataLoader(tf.keras.utils.Sequence):\n","    def __init__(self, data, img_dim=256, batch=32, path='/'): \n","        self.data = data\n","        self.img_dim = img_dim\n","        self.batch = batch\n","        self.path = path\n","        self.indexes = np.arange(len(self.data))\n","    \n","    def __getitem__(self, index):\n","        indexes = self.indexes[self.batch * index: self.batch * (index + 1)]\n","        X = self.__data_generation(indexes)\n","        return X\n","        \n","    def __data_generation(self, indexes): \n","        X = np.zeros((len(indexes), self.img_dim, self.img_dim, 3),\n","                        dtype=np.float32)\n","        data = self.data.iloc[indexes]\n","\n","        for i, (_, row) in enumerate(data.iterrows()):\n","            img = cv2.imread(self.path + row.image)\n","            X[i,] = cv2.resize(img, (self.img_dim, self.img_dim))\n","        return X\n","\n","    def __len__(self):\n","        n_batch = len(self.data) // self.batch\n","        n_batch += int(((len(self.data)) % self.batch) != 0)\n","        return n_batch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","from tensorflow.keras.applications import EfficientNetB0\n","\n","embeds = []\n","CHUNK = 1024 * 4\n","n_chunk = len(train) // CHUNK\n","if len(train) % CHUNK != 0:\n","    n_chunk += 1"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-20T14:16:16.335574Z","iopub.status.busy":"2022-04-20T14:16:16.334977Z","iopub.status.idle":"2022-04-20T14:21:40.614850Z","shell.execute_reply":"2022-04-20T14:21:40.613646Z","shell.execute_reply.started":"2022-04-20T14:16:16.335532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing image embeddings...\n","chunk 0 to 4096\n","128/128 [==============================] - 38s 290ms/step\n","chunk 8192 to 12288\n","128/128 [==============================] - 37s 281ms/step\n","chunk 12288 to 16384\n","128/128 [==============================] - 36s 268ms/step\n","chunk 16384 to 20480\n","128/128 [==============================] - 38s 288ms/step\n","chunk 20480 to 24576\n","128/128 [==============================] - 37s 281ms/step\n","chunk 24576 to 28672\n","128/128 [==============================] - 37s 285ms/step\n","chunk 28672 to 32768\n","128/128 [==============================] - 36s 278ms/step\n","chunk 32768 to 34250\n","47/47 [==============================] - 13s 266ms/step\n","image embeddings shape (34250, 1280)\n"]}],"source":["model = EfficientNetB0(weights='../input/effnetb0/efficientnetb0_notop.h5',\n","                       include_top=False,\n","                       pooling='avg',\n","                       input_shape=None,\n","                      )\n","\n","print('Computing image embeddings...')\n","for i, j in enumerate(range(n_chunk)):\n","    a = j * CHUNK\n","    b = (j + 1) * CHUNK\n","    b = min(b, len(train))\n","    print('chunk', a, 'to', b)\n","    \n","    train_gen = DataLoader(data=train.iloc[a:b],\n","                           batch_size=32,\n","                           path='../input/shopee-product-matching/train_images/')\n","    image_embeddings = model.predict(train_gen, verbose=1)\n","\n","    embeds.append(image_embeddings)\n","    \n","del model\n","_ = gc.collect()\n","image_embeddings = np.concatenate(embeds)\n","\n","print('image embeddings shape',image_embeddings.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Clustering(Kmeans & DBSCAN)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T14:21:40.625371Z","iopub.status.busy":"2022-04-20T14:21:40.624995Z","iopub.status.idle":"2022-04-20T14:21:40.676555Z","shell.execute_reply":"2022-04-20T14:21:40.673757Z","shell.execute_reply.started":"2022-04-20T14:21:40.625326Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","def kmeans_fit_match(train, embeddings, n_dims=200, n_clusters=50, max_n=50):\n","    train_0 = train.copy()\n","    embed_0 = embeddings.copy()\n","    \n","    print('fitting kmeans using {} samples and params: n_dims={}, n_clusters={}'\\\n","        .format(len(train_0), n_dims, n_clusters))\n","\n","    kmeans = KMeans(n_clusters).fit(embed_0[:, :n_dims])\n","    train_0['clusters'] = kmeans.labels_\n","\n","    clustered = (train_0['clusters'] != -1)\n","    pid_mapping = train_0.loc[clustered].groupby('clusters')['posting_id']\\\n","                    .agg('unique').to_dict()\n","    pid_mapping[-1] = []\n","    for key in pid_mapping:\n","        if len(pid_mapping[key]) > max_n:\n","            pid_mapping[key] = pid_mapping[key][:max_n]\n","    train_0['matches_kmeans'] = train_0['clusters'].map(pid_mapping)\\\n","                                        .apply(match_self, axis=1)\n","    \n","    train_0['f1_kmeans'] = train_0.apply(f1_score('matches_kmeans'), axis=1)\n","    return db, train_0['clusters'], train_0['matches_kmeans'], train_0['f1_kmeans']\n","\n","def match_self(row):\n","    if row['posting_id'] not in row['matches_kmeans']:\n","        return [row['posting_id']] + row['matches_kmeans']\n","    else:\n","        return row['matches_kmeans']\n","\n","def combine_for_cv(row, match_cols):\n","    x = np.concatenate([row[col] for col in match_cols])\n","    return np.unique(x)\n","\n","def combine_for_sub(row, match_cols):\n","    x = np.concatenate([row[col] for col in match_cols])\n","    return ' '.join(np.unique(x))"]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-20T14:21:40.683436Z","iopub.status.busy":"2022-04-20T14:21:40.683045Z","iopub.status.idle":"2022-04-20T14:21:40.826782Z","shell.execute_reply":"2022-04-20T14:21:40.825396Z","shell.execute_reply.started":"2022-04-20T14:21:40.683373Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import DBSCAN\n","\n","def dbscan_fit_match(train, embeddings, n_dims=200, eps=2, min_samp=2, max_n=50):\n","    train_0 = train.copy()\n","    embed_0 = embeddings.copy()\n","    \n","    print('fitting dbscan using {} samples and params: n_dims={}, eps={}'\\\n","        .format(len(train_0), n_dims, eps))\n","\n","    db = DBSCAN(eps=eps, \n","                min_samples=min_samp, \n","                metric='euclidean', \n","                n_jobs=-1).fit(embed_0[:, :n_dims])\n","    train_0['clusters'] = db.labels_\n","\n","    clustered = (train_0['clusters'] != -1)\n","    pid_mapping = train_0.loc[clustered].groupby('clusters')['posting_id']\\\n","            .agg('unique').to_dict()\n","    pid_mapping[-1] = []\n","    for key in pid_mapping:\n","        if len(pid_mapping[key]) > max_n:\n","            pid_mapping[key] = pid_mapping[key][:max_n]\n","    train_0['matches_dbscan'] = train_0['clusters'].map(pid_mapping)\\\n","                                    .apply(match_self, axis=1)\n","    \n","    train_0['f1_dbscan'] = train_0.apply(f1_score('matches_dbscan'), axis=1)\n","    return db, train_0['clusters'], train_0['matches_dbscan'], train_0['f1_dbscan']"]},{"cell_type":"markdown","metadata":{},"source":["### Optimize eps param on euclidean distance"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-20T14:21:40.831685Z","iopub.status.busy":"2022-04-20T14:21:40.831267Z","iopub.status.idle":"2022-04-20T14:21:40.965897Z","shell.execute_reply":"2022-04-20T14:21:40.964901Z","shell.execute_reply.started":"2022-04-20T14:21:40.831630Z"},"trusted":true},"outputs":[],"source":["sample_labels = pd.Series(train['label_group'].unique())\\\n","                    .sample(frac=.1, random_state=10)\n","train_sample = train[train['label_group'].isin(sample_labels)]\n","image_embeddings_sample = image_embeddings[train_sample.index]"]},{"cell_type":"code","execution_count":9,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-20T14:21:40.972359Z","iopub.status.busy":"2022-04-20T14:21:40.967393Z","iopub.status.idle":"2022-04-20T14:23:37.225017Z","shell.execute_reply":"2022-04-20T14:23:37.224123Z","shell.execute_reply.started":"2022-04-20T14:21:40.972319Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8999560527a54813a1c2d3b81acdcfe9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["fitting dbscan w/ eps=1...\n","fitting dbscan w/ eps=2...\n","fitting dbscan w/ eps=3...\n","fitting dbscan w/ eps=4...\n","fitting dbscan w/ eps=5...\n","fitting dbscan w/ eps=6...\n","fitting dbscan w/ eps=7...\n","fitting dbscan w/ eps=8...\n","fitting dbscan w/ eps=9...\n","fitting dbscan w/ eps=10...\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n_dims</th>\n","      <th>eps</th>\n","      <th>f1_phash</th>\n","      <th>f1_dbscan</th>\n","      <th>f1_combined</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1280</td>\n","      <td>1</td>\n","      <td>0.539179</td>\n","      <td>0.506883</td>\n","      <td>0.543739</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1280</td>\n","      <td>2</td>\n","      <td>0.539179</td>\n","      <td>0.542535</td>\n","      <td>0.553886</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1280</td>\n","      <td>3</td>\n","      <td>0.539179</td>\n","      <td>0.571889</td>\n","      <td>0.573918</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1280</td>\n","      <td>4</td>\n","      <td>0.539179</td>\n","      <td>0.597058</td>\n","      <td>0.596882</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1280</td>\n","      <td>5</td>\n","      <td>0.539179</td>\n","      <td>0.615709</td>\n","      <td>0.614902</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1280</td>\n","      <td>6</td>\n","      <td>0.539179</td>\n","      <td>0.638603</td>\n","      <td>0.637546</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1280</td>\n","      <td>7</td>\n","      <td>0.539179</td>\n","      <td>0.651855</td>\n","      <td>0.650711</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1280</td>\n","      <td>8</td>\n","      <td>0.539179</td>\n","      <td>0.633975</td>\n","      <td>0.634352</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1280</td>\n","      <td>9</td>\n","      <td>0.539179</td>\n","      <td>0.594504</td>\n","      <td>0.598462</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1280</td>\n","      <td>10</td>\n","      <td>0.539179</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   n_dims  eps  f1_phash  f1_dbscan  f1_combined\n","0    1280    1  0.539179   0.506883     0.543739\n","1    1280    2  0.539179   0.542535     0.553886\n","2    1280    3  0.539179   0.571889     0.573918\n","3    1280    4  0.539179   0.597058     0.596882\n","4    1280    5  0.539179   0.615709     0.614902\n","5    1280    6  0.539179   0.638603     0.637546\n","6    1280    7  0.539179   0.651855     0.650711\n","7    1280    8  0.539179   0.633975     0.634352\n","8    1280    9  0.539179   0.594504     0.598462\n","9    1280   10  0.539179   0.512375     0.522015"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["best dbscan f1 score = 0.651854747805969\n"]},{"data":{"text/plain":["-1      1762\n"," 134      29\n"," 288      23\n"," 34       21\n"," 137      18\n","        ... \n"," 89        2\n"," 91        2\n"," 103       2\n"," 113       2\n"," 0         2\n","Name: clusters, Length: 547, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["from tqdm.notebook import tqdm\n","\n","n_dims = 1280\n","eps_range = np.arange(1, 11, 1)\n","min_samp = 2\n","\n","opt_matrix = []\n","for eps in tqdm(eps_range, total=len(eps_range)):\n","    print('fitting dbscan w/ eps={}...'.format(round(eps, 3)))\n","    _, clusters, train_sample['matches_dbscan'], train_sample['f1_dbscan'] = dbscan_fit_match(train_sample, image_embeddings_sample, n_dims=n_dims, eps=eps, min_samp=min_samp, show_vc=False, verbose=False, metric='euclidean')\n","\n","    train_sample['matches'] = train_sample.apply(combine_for_cv, \n","                                                 axis=1, \n","                                                 match_cols=['matches_phash', 'matches_dbscan'])\n","    train_sample['f1_combined'] = train_sample.apply(f1_score('matches'),axis=1)\n","    opt_row = [n_dims,\n","               eps,\n","               clusters.value_counts(),\n","               train_sample['f1_phash'].mean(), \n","               train_sample['f1_dbscan'].mean(), \n","               train_sample['f1_combined'].mean()]\n","    opt_matrix.append(opt_row)\n","    \n","opt_df = pd.DataFrame(opt_matrix, columns=['n_dims', 'eps', 'counts', 'f1_phash', 'f1_dbscan', 'f1_combined'])\n","display(opt_df[['n_dims', 'eps', 'f1_phash', 'f1_dbscan', 'f1_combined']])\n","\n","print('best dbscan f1 score = {}'.format(opt_df['f1_dbscan'].max()))\n","display(opt_df.sort_values(by='f1_dbscan', ascending=False)['counts'].iloc[0])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-20T14:47:35.121789Z","iopub.status.busy":"2022-04-20T14:47:35.121445Z","iopub.status.idle":"2022-04-20T14:50:13.045443Z","shell.execute_reply":"2022-04-20T14:50:13.044505Z","shell.execute_reply.started":"2022-04-20T14:47:35.121759Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19a35f66bed74410a7b0408426dacc49","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["fitting kmeans w/ n_cluster=20...\n","fitting kmeans w/ n_cluster=25...\n","fitting kmeans w/ n_cluster=30...\n","fitting kmeans w/ n_cluster=35...\n","fitting kmeans w/ n_cluster=40...\n","fitting kmeans w/ n_cluster=45...\n","fitting kmeans w/ n_cluster=50...\n","fitting kmeans w/ n_cluster=55...\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>n_cluster</th>\n","      <th>counts</th>\n","      <th>f1_kmeans</th>\n","      <th>f1_combined</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>20</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>35</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>45</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>50</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>55</td>\n","      <td>-1      1003\n"," 1       789\n"," 18      146\n"," 0     ...</td>\n","      <td>0.512375</td>\n","      <td>0.522015</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   n_cluster                                             counts  f1_kmeans  \\\n","0         20  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","1         25  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","2         30  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","3         35  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","4         40  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","5         45  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","6         50  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","7         55  -1      1003\n"," 1       789\n"," 18      146\n"," 0     ...   0.512375   \n","\n","   f1_combined  \n","0     0.522015  \n","1     0.522015  \n","2     0.522015  \n","3     0.522015  \n","4     0.522015  \n","5     0.522015  \n","6     0.522015  \n","7     0.522015  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["best dbscan f1 score = 0.5123750414148504\n"]},{"data":{"text/plain":["-1      1003\n"," 1       789\n"," 18      146\n"," 0        42\n"," 33       33\n","        ... \n"," 51        2\n"," 53        2\n"," 55        2\n"," 57        2\n"," 445       2\n","Name: clusters, Length: 447, dtype: int64"]},"metadata":{},"output_type":"display_data"}],"source":["n_dims=1280\n","n_clusters_lst = np.arange(20, 60, 5)\n","\n","opt_matrix = []\n","\n","for n_cluster in tqdm(n_clusters_lst, total=len(n_clusters_lst)):\n","    print('fitting kmeans w/ n_cluster={}...'.format(round(n_cluster, 3)))\n","    _, clusters, train_sample['matches_kmeans'], train_sample['f1_kmeans'] = kmeans_fit_match(train_sample,\n","                                                                                                image_embeddings_sample,\n","                                                                                                n_dims=n_dims,\n","                                                                                                n_clusters=n_cluster,\n","                                                                                                show_vc=False,\n","                                                                                                verbose=False)\n","\n","    # combine dbscan matches and pHash matches\n","    train_sample['matches'] = train_sample.apply(combine_for_cv, \n","                                                 axis=1, \n","                                                 match_cols=['matches_phash', 'matches_kmeans'])\n","    train_sample['f1_combined'] = train_sample.apply(f1_score('matches'),axis=1)\n","    opt_row = [n_cluster,\n","                clusters.value_counts(),\n","                train_sample['f1_kmeans'].mean(), \n","                train_sample['f1_combined'].mean()]\n","    opt_matrix.append(opt_row)\n","\n","opt_df = pd.DataFrame(opt_matrix, columns=['n_cluster', 'counts', 'f1_kmeans', 'f1_combined'])\n","display(opt_df[['n_cluster', 'counts', 'f1_kmeans', 'f1_combined']])\n","print('best dbscan f1 score = {}'.format(opt_df['f1_kmeans'].max()))\n","display(opt_df.sort_values(by='f1_kmeans', ascending=False)['counts'].iloc[0])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
